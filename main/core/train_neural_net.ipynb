{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a neural net model to work with the Butters web application.\n",
    "\n",
    "I followed the following tutorial to use transfer learning:\n",
    "https://www.youtube.com/watch?v=6LXKugY5bFU\n",
    "\n",
    "If I weren't so lazy I would label my own youtube dataset. But I am lazy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modeling\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# Huggingface Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Import accuracy_score to check performance\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset has a length of 800 records.\n",
      "The test dataset has a length of 200 records.\n"
     ]
    }
   ],
   "source": [
    "# Read file\n",
    "df_amz = pd.read_csv('amazon_cells_labelled.txt', sep='\\t', names=['review', 'label'])\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_amz['review'],\n",
    "                                                    df_amz['label'],\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(f'The train dataset has a length of {len(X_train)} records.')\n",
    "print(f'The test dataset has a length of {len(X_test)} records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101 17554   112   189  2080  2965   119   102     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer from pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Tokenize the reviews\n",
    "tokenized_data_train = tokenizer(X_train.to_list(), return_tensors='np', padding=True)\n",
    "tokenized_data_test = tokenizer(X_test.to_list(), return_tensors='np', padding=True)\n",
    "\n",
    "# Labels are one-dimensional numpy array\n",
    "labels_train = np.array(y_train)\n",
    "labels_test = np.array(y_test)\n",
    "\n",
    "# Tokenized ids\n",
    "print(tokenized_data_train['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55cc53b4601a4736ac502ea5e4465c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qmeye\\OneDrive\\Documents\\Projects\\Butters\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\qmeye\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(5e-6), loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "200/200 [==============================] - 231s 1s/step - loss: 0.5749 - accuracy: 0.7150 - val_loss: 0.3159 - val_accuracy: 0.9150\n",
      "Epoch 2/2\n",
      "200/200 [==============================] - 199s 997ms/step - loss: 0.2206 - accuracy: 0.9350 - val_loss: 0.2000 - val_accuracy: 0.9400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2410ce17640>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(dict(tokenized_data_train),\n",
    "    labels_train,\n",
    "    validation_data=(dict(tokenized_data_test), labels_test),\n",
    "    batch_size=4,\n",
    "    epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 9s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.3897868,  1.4423205],\n",
       "       [-1.2231984,  1.3496759],\n",
       "       [-1.5523567,  1.5151092],\n",
       "       [ 2.1606205, -1.8553412],\n",
       "       [-1.4364722,  1.4405607]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions\n",
    "y_test_predict = model.predict(dict(tokenized_data_test))['logits']\n",
    "\n",
    "# First 5 predictions\n",
    "y_test_predict[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[0.05561361, 0.94438636],\n",
       "       [0.07090472, 0.92909527],\n",
       "       [0.04446939, 0.9555306 ],\n",
       "       [0.98229355, 0.01770644],\n",
       "       [0.05330066, 0.9466993 ]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted probablities (apply softmax to get probabliites that add up to 1)\n",
    "y_test_probabilities = tf.nn.softmax(y_test_predict)\n",
    "\n",
    "# First 5 probabiltiies\n",
    "y_test_probabilities[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted Label\n",
    "y_test_class_preds = np.argmax(y_test_probabilities, axis=1)\n",
    "\n",
    "# First 5 labels\n",
    "y_test_class_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy of validation data\n",
    "accuracy_score(y_test_class_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('./sentiment_transfer_learning_tensorflow/')\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained('./sentiment_transfer_learning_tensorflow/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./sentiment_transfer_learning_tensorflow/ were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ./sentiment_transfer_learning_tensorflow/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Verify model works\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./sentiment_transfer_learning_tensorflow/')\n",
    "\n",
    "# Load model\n",
    "loaded_model = TFAutoModelForSequenceClassification.from_pretrained('./sentiment_transfer_learning_tensorflow/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 8s 868ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.3897868,  1.4423205],\n",
       "       [-1.2231984,  1.3496759],\n",
       "       [-1.5523567,  1.5151092],\n",
       "       [ 2.1606205, -1.8553412],\n",
       "       [-1.4364722,  1.4405607]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict logit using the loaded model\n",
    "y_test_predict = loaded_model.predict(dict(tokenized_data_test))['logits']\n",
    "\n",
    "# First 5 predictions\n",
    "y_test_predict[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
